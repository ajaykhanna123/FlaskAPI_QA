{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb141a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import CTransformers\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1aa1f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download model here https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/blob/main/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
    "loader = TextLoader(\"llama2_text.txt\")\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3f6afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "                                             chunk_size=500,\n",
    "                                             chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00bc87fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_splitter.split_documents(documents)\n",
    "\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56b3df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device':'cpu'})\n",
    "\n",
    "\n",
    "#**Step 4: Convert the Text Chunks into Embeddings and Create a FAISS Vector Store***\n",
    "vector_store=FAISS.from_documents(text_chunks, embeddings)\n",
    "\n",
    "\n",
    "##**Step 5: Find the Top 3 Answers for the Query***\n",
    "\n",
    "query=\"YOLOv7 outperforms which models\"\n",
    "docs = vector_store.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60c54a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=None, metadata=None, vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x00000209BDAEF6D0>, search_type='similarity', search_kwargs={'k': 2})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.as_retriever(search_kwargs={'k': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17ca39d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=\"llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':500,\n",
    "                          'temperature':0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00c96b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you dont know the answer just say you know, don't try to make up an answer.\n",
    "\n",
    "Context:{context}\n",
    "Question:{question}\n",
    "\n",
    "Only return the helpful answer below and nothing else\n",
    "Helpful answer\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt=PromptTemplate(template=template, input_variables=['context', 'question'])\n",
    "\n",
    "#start=timeit.default_timer()\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                   chain_type='stuff',\n",
    "                                   retriever=vector_store.as_retriever(search_kwargs={'k': 5}),\n",
    "                                   return_source_documents=True,\n",
    "                                   chain_type_kwargs={'prompt': qa_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f04267d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, combine_documents_chain=StuffDocumentsChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"Use the following pieces of information to answer the user's question.\\nIf you dont know the answer just say you know, don't try to make up an answer.\\n\\nContext:{context}\\nQuestion:{question}\\n\\nOnly return the helpful answer below and nothing else\\nHelpful answer\\n\", template_format='f-string', validate_template=True), llm=CTransformers(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<ctransformers.llm.LLM object at 0x00000209BDAEF5B0>, model='llama-2-7b-chat.ggmlv3.q4_0.bin', model_type='llama', model_file=None, config={'max_new_tokens': 500, 'temperature': 0.1}, lib=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), document_variable_name='context', document_separator='\\n\\n'), input_key='query', output_key='result', return_source_documents=True, retriever=VectorStoreRetriever(tags=None, metadata=None, vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x00000209BDAEF6D0>, search_type='similarity', search_kwargs={'k': 5}))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = datetime.datetime.now()\n",
    "question=\"when was llama announced?\"\n",
    "chat_history=[]\n",
    "result=chain({'query':question,\"chat_history\": chat_history})\n",
    "print(result['result'])\n",
    "b = datetime.datetime.now()\n",
    "print(\"time taken -\",b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bd956",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = datetime.datetime.now()\n",
    "question=\"What is length of  Llama 2 - Chat? \"\n",
    "chat_history=[]\n",
    "result=chain({'query':question,\"chat_history\": chat_history})\n",
    "print(result['result'])\n",
    "b = datetime.datetime.now()\n",
    "print(\"time taken -\",b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f759ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84b03e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python310",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
